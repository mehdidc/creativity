{
 "metadata": {
  "name": "",
  "signature": "sha256:c452af8cbcee71bbb361c22c00751bc23dbc9d1879d19db43a4802a43673ae23"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      " This tutorial introduces denoising auto-encoders (dA) using Theano.\n",
      "\n",
      " Denoising autoencoders are the building blocks for SdA.\n",
      " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
      " An autoencoder takes an input x and first maps it to a hidden representation\n",
      " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
      " latent representation y is then mapped back to a \"reconstructed\" vector\n",
      " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
      " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
      " the autoencoder is said to have tied weights. The network is trained such\n",
      " that to minimize the reconstruction error (the error between x and z).\n",
      "\n",
      " For the denosing autoencoder, during training, first x is corrupted into\n",
      " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
      " of a stochastic mapping. Afterwards y is computed as before (using\n",
      " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
      " error is now measured between z and the uncorrupted input x, which is\n",
      " computed as the cross-entropy :\n",
      "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
      "\n",
      "\n",
      " References :\n",
      "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
      "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
      "   2008\n",
      "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
      "   Training of Deep Networks, Advances in Neural Information Processing\n",
      "   Systems 19, 2007\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from inc.logistic_sgd import load_data\n",
      "from inc.utils import tile_raster_images\n",
      "\n",
      "try:\n",
      "    import PIL.Image as Image\n",
      "except ImportError:\n",
      "    import Image\n",
      "\n",
      "\n",
      "# start-snippet-1\n",
      "class dA(object):\n",
      "    \"\"\"Denoising Auto-Encoder class (dA)\n",
      "\n",
      "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
      "    version of it by projecting it first in a latent space and reprojecting\n",
      "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
      "    for more details. If x is the input then equation (1) computes a partially\n",
      "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
      "    computes the projection of the input into the latent space. Equation (3)\n",
      "    computes the reconstruction of the input, while equation (4) computes the\n",
      "    reconstruction error.\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
      "\n",
      "        y = s(W \\tilde{x} + b)                                           (2)\n",
      "\n",
      "        x = s(W' y  + b')                                                (3)\n",
      "\n",
      "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        numpy_rng,\n",
      "        theano_rng=None,\n",
      "        input=None,\n",
      "        n_visible=784,\n",
      "        n_hidden=500,\n",
      "        W=None,\n",
      "        bhid=None,\n",
      "        bvis=None\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the dA class by specifying the number of visible units (the\n",
      "        dimension d of the input ), the number of hidden units ( the dimension\n",
      "        d' of the latent or hidden space ) and the corruption level. The\n",
      "        constructor also receives symbolic variables for the input, weights and\n",
      "        bias. Such a symbolic variables are useful when, for example the input\n",
      "        is the result of some computations, or when weights are shared between\n",
      "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
      "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
      "        and the weights of the dA are used in the second stage of training\n",
      "        to construct an MLP.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: number random generator used to generate weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                     generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: a symbolic description of the input or None for\n",
      "                      standalone dA\n",
      "\n",
      "        :type n_visible: int\n",
      "        :param n_visible: number of visible units\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden:  number of hidden units\n",
      "\n",
      "        :type W: theano.tensor.TensorType\n",
      "        :param W: Theano variable pointing to a set of weights that should be\n",
      "                  shared belong the dA and another architecture; if dA should\n",
      "                  be standalone set this to None\n",
      "\n",
      "        :type bhid: theano.tensor.TensorType\n",
      "        :param bhid: Theano variable pointing to a set of biases values (for\n",
      "                     hidden units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "        :type bvis: theano.tensor.TensorType\n",
      "        :param bvis: Theano variable pointing to a set of biases values (for\n",
      "                     visible units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "\n",
      "        \"\"\"\n",
      "        self.n_visible = n_visible\n",
      "        self.n_hidden = n_hidden\n",
      "\n",
      "        # create a Theano random generator that gives symbolic random values\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
      "        if not W:\n",
      "            # W is initialized with `initial_W` which is uniformely sampled\n",
      "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
      "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
      "            # converted using asarray to dtype\n",
      "            # theano.config.floatX so that the code is runable on GPU\n",
      "            initial_W = numpy.asarray(\n",
      "                numpy_rng.uniform(\n",
      "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    size=(n_visible, n_hidden)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
      "\n",
      "        if not bvis:\n",
      "            bvis = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_visible,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        if not bhid:\n",
      "            bhid = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_hidden,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                name='b',\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        self.W = W\n",
      "        # b corresponds to the bias of the hidden\n",
      "        self.b = bhid\n",
      "        # b_prime corresponds to the bias of the visible\n",
      "        self.b_prime = bvis\n",
      "        # tied weights, therefore W_prime is W transpose\n",
      "        \n",
      "        \n",
      "        initial_W_t = numpy.asarray(\n",
      "            numpy_rng.uniform(\n",
      "                low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                size=(n_visible, n_hidden)\n",
      "            ),\n",
      "            dtype=theano.config.floatX\n",
      "        )\n",
      "        W_t = theano.shared(value=initial_W.T, name='W_t', borrow=True)\n",
      "        \n",
      "        self.W_prime = self.W.T\n",
      "        #self.W_prime = W_t\n",
      "        \n",
      "        \n",
      "        self.theano_rng = theano_rng\n",
      "        # if no input is given, generate a variable representing the input\n",
      "        if input is None:\n",
      "            # we use a matrix because we expect a minibatch of several\n",
      "            # examples, each example being a row\n",
      "            self.x = T.dmatrix(name='input')\n",
      "        else:\n",
      "            self.x = input\n",
      "\n",
      "        self.params = [self.W, self.b, self.b_prime]\n",
      "    # end-snippet-1\n",
      "\n",
      "    def get_corrupted_input(self, input, corruption_level):\n",
      "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
      "        same and zero-out randomly selected subset of size ``coruption_level``\n",
      "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
      "               random numbers that it should produce\n",
      "               second argument is the number of trials\n",
      "               third argument is the probability of success of any trial\n",
      "\n",
      "                this will produce an array of 0s and 1s where 1 has a\n",
      "                probability of 1 - ``corruption_level`` and 0 with\n",
      "                ``corruption_level``\n",
      "\n",
      "                The binomial function return int64 data type by\n",
      "                default.  int64 multiplicated by the input\n",
      "                type(floatX) always return float64.  To keep all data\n",
      "                in floatX when floatX is float32, we set the dtype of\n",
      "                the binomial to floatX. As in our case the value of\n",
      "                the binomial is always 0 or 1, this don't change the\n",
      "                result. This is needed to allow the gpu to work\n",
      "                correctly as it only support float32 for now.\n",
      "\n",
      "        \"\"\"\n",
      "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
      "                                        p=1 - corruption_level,\n",
      "                                        dtype=theano.config.floatX) * input\n",
      "\n",
      "    def get_hidden_values(self, input):\n",
      "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "    def get_reconstructed_input(self, hidden):\n",
      "        \"\"\"Computes the reconstructed input given the values of the\n",
      "        hidden layer\n",
      "\n",
      "        \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
      "\n",
      "    def get_cost_updates(self, corruption_level, learning_rate):\n",
      "        \"\"\" This function computes the cost and the updates for one trainng\n",
      "        step of the dA \"\"\"\n",
      "\n",
      "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
      "        y = self.get_hidden_values(tilde_x)\n",
      "        z = self.get_reconstructed_input(y)\n",
      "        # note : we sum over the size of a datapoint; if we are using\n",
      "        #        minibatches, L will be a vector, with one entry per\n",
      "        #        example in minibatch\n",
      "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
      "        # note : L is now a vector, where each element is the\n",
      "        #        cross-entropy cost of the reconstruction of the\n",
      "        #        corresponding example of the minibatch. We need to\n",
      "        #        compute the average of all these to get the cost of\n",
      "        #        the minibatch\n",
      "        cost = T.mean(L)\n",
      "\n",
      "        # compute the gradients of the cost of the `dA` with respect\n",
      "        # to its parameters\n",
      "        gparams = T.grad(cost, self.params)\n",
      "        # generate the list of updates\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(self.params, gparams)\n",
      "        ]\n",
      "\n",
      "        return (cost, updates)\n",
      "\n",
      "\n",
      "def test_dA(learning_rate=0.1, training_epochs=15,\n",
      "            dataset='mnist.pkl.gz',\n",
      "            batch_size=20, output_folder='dA_plots'):\n",
      "\n",
      "    \"\"\"\n",
      "    This demo is tested on MNIST\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used for training the DeNosing\n",
      "                          AutoEncoder\n",
      "\n",
      "    :type training_epochs: int\n",
      "    :param training_epochs: number of epochs used for training\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: path to the picked dataset\n",
      "\n",
      "    \"\"\"\n",
      "    datasets = load_data(os.path.join(os.getenv(\"DATA_PATH\"), \"mnist\", dataset))\n",
      "    train_set_x, train_set_y = datasets[0]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()    # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "\n",
      "    if not os.path.isdir(output_folder):\n",
      "        os.makedirs(output_folder)\n",
      "    os.chdir(output_folder)\n",
      "    ####################################\n",
      "    # BUILDING THE MODEL NO CORRUPTION #\n",
      "    ####################################\n",
      "\n",
      "    rng = numpy.random.RandomState(123)\n",
      "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
      "\n",
      "    da = dA(\n",
      "        numpy_rng=rng,\n",
      "        theano_rng=theano_rng,\n",
      "        input=x,\n",
      "        n_visible=28 * 28,\n",
      "        n_hidden=500\n",
      "    )\n",
      "\n",
      "    cost, updates = da.get_cost_updates(\n",
      "        corruption_level=0.,\n",
      "        learning_rate=learning_rate\n",
      "    )\n",
      "\n",
      "    train_da = theano.function(\n",
      "        [index],\n",
      "        cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    ############\n",
      "    # TRAINING #\n",
      "    ############\n",
      "\n",
      "    # go through training epochs\n",
      "    for epoch in xrange(training_epochs):\n",
      "        # go through trainng set\n",
      "        c = []\n",
      "        for batch_index in xrange(n_train_batches):\n",
      "            c.append(train_da(batch_index))\n",
      "\n",
      "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    training_time = (end_time - start_time)\n",
      "\n",
      "    #print >> sys.stderr, ('The no corruption code for file ' +\n",
      "    #                      os.path.split(__file__)[1] +\n",
      "    #                      ' ran for %.2fm' % ((training_time) / 60.))\n",
      "    image = Image.fromarray(\n",
      "        tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
      "                           img_shape=(28, 28), tile_shape=(20, 20),\n",
      "                           tile_spacing=(1, 1)))\n",
      "    image.save('filters_corruption_0.png')\n",
      "\n",
      "    #####################################\n",
      "    # BUILDING THE MODEL CORRUPTION 30% #\n",
      "    #####################################\n",
      "\n",
      "    rng = numpy.random.RandomState(123)\n",
      "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
      "\n",
      "    da = dA(\n",
      "        numpy_rng=rng,\n",
      "        theano_rng=theano_rng,\n",
      "        input=x,\n",
      "        n_visible=28 * 28,\n",
      "        n_hidden=500\n",
      "    )\n",
      "\n",
      "    cost, updates = da.get_cost_updates(\n",
      "        corruption_level=0.3,\n",
      "        learning_rate=learning_rate\n",
      "    )\n",
      "\n",
      "    train_da = theano.function(\n",
      "        [index],\n",
      "        cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    start_time = time.clock()\n",
      "\n",
      "    ############\n",
      "    # TRAINING #\n",
      "    ############\n",
      "\n",
      "    # go through training epochs\n",
      "    for epoch in xrange(training_epochs):\n",
      "        # go through trainng set\n",
      "        c = []\n",
      "        for batch_index in xrange(n_train_batches):\n",
      "            c.append(train_da(batch_index))\n",
      "\n",
      "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
      "\n",
      "    end_time = time.clock()\n",
      "\n",
      "    training_time = (end_time - start_time)\n",
      "\n",
      "    #print >> sys.stderr, ('The 30% corruption code for file ' +\n",
      "    #                      os.path.split(__file__)[1] +\n",
      "    #                      ' ran for %.2fm' % (training_time / 60.))\n",
      "\n",
      "    image = Image.fromarray(tile_raster_images(\n",
      "        X=da.W.get_value(borrow=True).T,\n",
      "        img_shape=(28, 28), tile_shape=(20, 20),\n",
      "        tile_spacing=(1, 1)))\n",
      "    image.save('filters_corruption_30.png')\n",
      "\n",
      "    os.chdir('../')\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... loading data\n",
        "Training epoch 0, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 63.2892\n",
        "Training epoch 1, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 55.7867\n",
        "Training epoch 2, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 54.7631\n",
        "Training epoch 3, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 54.2421\n",
        "Training epoch 4, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53.8887\n",
        "Training epoch 5, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53.6203\n",
        "Training epoch 6, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53.4038\n",
        "Training epoch 7, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53.222\n",
        "Training epoch 8, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53.0658\n",
        "Training epoch 9, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.9296\n",
        "Training epoch 10, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.8094\n",
        "Training epoch 11, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.7024\n",
        "Training epoch 12, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.6063\n",
        "Training epoch 13, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.5192\n",
        "Training epoch 14, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.4395\n",
        "Training epoch 0, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 81.7714\n",
        "Training epoch 1, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 73.4286\n",
        "Training epoch 2, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 70.8633\n",
        "Training epoch 3, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 69.3397\n",
        "Training epoch 4, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 68.4135\n",
        "Training epoch 5, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 67.7237\n",
        "Training epoch 6, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 67.2401\n",
        "Training epoch 7, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66.8493\n",
        "Training epoch 8, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66.5664\n",
        "Training epoch 9, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66.3591\n",
        "Training epoch 10, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66.1337\n",
        "Training epoch 11, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65.9894\n",
        "Training epoch 12, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65.8344\n",
        "Training epoch 13, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65.7185\n",
        "Training epoch 14, cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65.6011\n"
       ]
      }
     ],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}