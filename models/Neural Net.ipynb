{
 "metadata": {
  "name": "",
  "signature": "sha256:b4b0eb0197cc1387a44af1a63169889137a6b186329cab13ed0c75114a4e0ab7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import matplotlib\n",
      "# Force matplotlib to not use any Xwindows backend.\n",
      "matplotlib.use('Agg')\n",
      "import numpy as np\n",
      " \n",
      "class NeuralNetwork(object):\n",
      "    def __init__(self, X, y, parameters):\n",
      "        #Input data\n",
      "        self.X=X\n",
      "        #Output data\n",
      "        self.y=y\n",
      "        #Expect parameters to be a tuple of the form:\n",
      "        #    ((n_input,0,0), (n_hidden_layer_1, f_1, f_1'), ...,\n",
      "        #     (n_hidden_layer_k, f_k, f_k'), (n_output, f_o, f_o'))\n",
      "        self.n_layers = len(parameters)\n",
      "        #Counts number of neurons without bias neurons in each layer.\n",
      "        self.sizes = [layer[0] for layer in parameters]\n",
      "        #Activation functions for each layer.\n",
      "        self.fs =[layer[1] for layer in parameters]\n",
      "        #Derivatives of activation functions for each layer.\n",
      "        self.fprimes = [layer[2] for layer in parameters]\n",
      "        self.build_network()\n",
      " \n",
      "    def build_network(self):\n",
      "        #List of weight matrices taking the output of one layer to the input of the next.\n",
      "        self.weights=[]\n",
      "        #Bias vector for each layer.\n",
      "        self.biases=[]\n",
      "        #Input vector for each layer.\n",
      "        self.inputs=[]\n",
      "        #Output vector for each layer.\n",
      "        self.outputs=[]\n",
      "        #Vector of errors at each layer.\n",
      "        self.errors=[]\n",
      "        #We initialise the weights randomly, and fill the other vectors with 1s.\n",
      "        for layer in range(self.n_layers-1):\n",
      "            n = self.sizes[layer]\n",
      "            m = self.sizes[layer+1]\n",
      "            self.weights.append(np.random.normal(0,1, (m,n)))\n",
      "            self.biases.append(np.random.normal(0,1,(m,1)))\n",
      "            self.inputs.append(np.zeros((n,1)))\n",
      "            self.outputs.append(np.zeros((n,1)))\n",
      "            self.errors.append(np.zeros((n,1)))\n",
      "        #There are only n-1 weight matrices, so we do the last case separately.\n",
      "        n = self.sizes[-1]\n",
      "        self.inputs.append(np.zeros((n,1)))\n",
      "        self.outputs.append(np.zeros((n,1)))\n",
      "        self.errors.append(np.zeros((n,1)))\n",
      " \n",
      "    def feedforward(self, x):\n",
      "        #Propagates the input from the input layer to the output layer.\n",
      "        k=len(x)\n",
      "        x.shape=(k,1)\n",
      "        self.inputs[0]=x\n",
      "        self.outputs[0]=x\n",
      "        for i in range(1,self.n_layers):\n",
      "            self.inputs[i]=self.weights[i-1].dot(self.outputs[i-1])+self.biases[i-1]\n",
      "            self.outputs[i]=self.fs[i](self.inputs[i])\n",
      "        return self.outputs[-1]\n",
      " \n",
      "    def update_weights(self,x,y):\n",
      "        #Update the weight matrices for each layer based on a single input x and target y.\n",
      "        output = self.feedforward(x)\n",
      "        self.errors[-1]=self.fprimes[-1](self.outputs[-1])*(output-y)\n",
      " \n",
      "        n=self.n_layers-2\n",
      "        for i in xrange(n,0,-1):\n",
      "            self.errors[i] = self.fprimes[i](self.inputs[i])*self.weights[i].T.dot(self.errors[i+1])\n",
      "            self.weights[i] = self.weights[i]-self.learning_rate*np.outer(self.errors[i+1],self.outputs[i])\n",
      "            self.biases[i] = self.biases[i] - self.learning_rate*self.errors[i+1]\n",
      "        self.weights[0] = self.weights[0]-self.learning_rate*np.outer(self.errors[1],self.outputs[0])\n",
      "        self.biases[0] = self.biases[0] - self.learning_rate*self.errors[1] \n",
      "    def train(self,n_iter, learning_rate=1):\n",
      "        #Updates the weights after comparing each input in X with y\n",
      "        #repeats this process n_iter times.\n",
      "        self.learning_rate=learning_rate\n",
      "        n=self.X.shape[0]\n",
      "        for repeat in range(n_iter):\n",
      "            #We shuffle the order in which we go through the inputs on each iter.\n",
      "            index=list(range(n))\n",
      "            np.random.shuffle(index)\n",
      "            for row in index:\n",
      "                x=self.X[row]\n",
      "                y=self.y[row]\n",
      "                self.update_weights(x,y)\n",
      " \n",
      "    def predict_x(self, x):\n",
      "        return self.feedforward(x)\n",
      " \n",
      "    def predict(self, X):\n",
      "        n = len(X)\n",
      "        m = self.sizes[-1]\n",
      "        ret = np.ones((n,m))\n",
      "        for i in range(len(X)):\n",
      "            ret[i,:] = self.feedforward(X[i])\n",
      "        return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic(x):\n",
      "    return 1.0/(1+np.exp(-x))\n",
      " \n",
      "def logistic_prime(x):\n",
      "    ex=np.exp(-x)\n",
      "    return ex/(1+ex)**2\n",
      " \n",
      "def identity(x):\n",
      "    return x\n",
      " \n",
      "def identity_prime(x):\n",
      "    return 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#expit is a fast way to compute logistic using precomputed exp.\n",
      "from scipy.special import expit\n",
      "def test_regression(plots=False):\n",
      "    #First create the data.\n",
      "    n=200\n",
      "    X=np.linspace(0,3*np.pi,num=n)\n",
      "    X.shape=(n,1)\n",
      "    y=np.sin(X)\n",
      "    #We make a neural net with 2 hidden layers, 20 neurons in each, using logistic activation\n",
      "    #functions.\n",
      "    param=((1,0,0),(20, expit, logistic_prime),(20, expit, logistic_prime),(1,identity, identity_prime))\n",
      "    #Set learning rate.\n",
      "    rates=[0.05]\n",
      "    predictions=[]\n",
      "    for rate in rates:\n",
      "        N=NeuralNetwork(X,y,param)\n",
      "        N.train(4000, learning_rate=rate)\n",
      "        predictions.append([rate,N.predict(X)])\n",
      "  \n",
      "    fig, ax=plt.subplots(1,1)\n",
      "    if plots:\n",
      "        ax.plot(X,y, label='Sine', linewidth=2, color='black')\n",
      "        for data in predictions:\n",
      "            ax.plot(X,data[1],label=\"Learning Rate: \"+str(data[0]))\n",
      "        ax.legend()\n",
      "test_regression(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-90a90c74d2f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Learning Rate: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtest_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-12-90a90c74d2f8>\u001b[0m in \u001b[0;36mtest_regression\u001b[1;34m(plots)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mplots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Sine'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_classification(plots=False):\n",
      "    #Number samples\n",
      "    n=700\n",
      " \n",
      "    n_iter=1500\n",
      "    learning_rate=0.05\n",
      " \n",
      "    #Samples for true decision boundary plot\n",
      "    L=np.linspace(0,3*np.pi,num=n)\n",
      "    l = np.sin(L)\n",
      " \n",
      "    #Data inputs, training\n",
      "    X = np.random.uniform(0, 3*np.pi, size=(n,2))\n",
      "    X[:,1] *= 1/np.pi\n",
      "    X[:,1]-= 1\n",
      " \n",
      "    #Data inputs, testing\n",
      "    T = np.random.uniform(0, 3*np.pi, size=(n,2))\n",
      "    T[:,1] *= 1/np.pi\n",
      "    T[:,1] -= 1\n",
      " \n",
      "    #Data outputs\n",
      "    y = np.sin(X[:,0]) <= X[:,1]\n",
      " \n",
      "    #Fitting\n",
      "    param=((2,0,0),(30, expit, logistic_prime),(30, expit, logistic_prime),(1,expit, logistic_prime))\n",
      "    N=NeuralNetwork(X,y, param)\n",
      "    #Training\n",
      "    N.train(n_iter, learning_rate)\n",
      "    predictions_training=N.predict(X)\n",
      "    predictions_training= predictions_training <0.5\n",
      "    predictions_training= predictions_training[:,0]\n",
      "    #Testing\n",
      "    predictions_testing=N.predict(T)\n",
      "    predictions_testing= predictions_testing <0.5\n",
      "    predictions_testing= predictions_testing[:,0]\n",
      " \n",
      "    #Plotting\n",
      "    matplotlib.use('Agg')\n",
      "    import matplotlib.pyplot as plt\n",
      "    fig, ax=plt.subplots(2,1)\n",
      " \n",
      "    #Training plot\n",
      "    #We plot the predictions of the neural net blue for class 0, red for 1.\n",
      "    ax[0].scatter(X[predictions_training,0], X[predictions_training,1], color='blue')\n",
      "    not_index = np.logical_not(predictions_training)\n",
      "    ax[0].scatter(X[not_index,0], X[not_index,1], color='red')\n",
      "    ax[0].set_xlim(0, 3*np.pi)\n",
      "    ax[0].set_ylim(-1,1)\n",
      "    #True decision boundary\n",
      "    ax[0].plot(L,l, color='black')\n",
      "    #Shade the areas according to how to they should be classified.\n",
      "    ax[0].fill_between(L, l,y2=-1, alpha=0.5)\n",
      "    ax[0].fill_between(L, l, y2=1, alpha=0.5, color='red')\n",
      " \n",
      "    #Testing plot\n",
      "    ax[1].scatter(T[predictions_testing,0], T[predictions_testing,1], color='blue')\n",
      "    not_index = np.logical_not(predictions_testing)\n",
      "    ax[1].scatter(T[not_index,0], T[not_index,1], color='red')\n",
      "    ax[1].set_xlim(0, 3*np.pi)\n",
      "    ax[1].set_ylim(-1,1)\n",
      "    ax[1].plot(L,l, color='black')\n",
      "    ax[1].fill_between(L, l,y2=-1, alpha=0.5)\n",
      "    ax[1].fill_between(L, l, y2=1, alpha=0.5, color='red')    \n",
      " \n",
      "test_classification()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}