{
 "metadata": {
  "name": "",
  "signature": "sha256:5e3a1d5957c2470243597096d565abda4d64162b48318d3595a3db6c6410002b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#%load_ext autoreload\n",
      "\n",
      "#%autoreload 2\n",
      "\n",
      "from lasagne import easy\n",
      "import lasagne\n",
      "import lasagne.data\n",
      "\n",
      "# currently available datasets:\n",
      "from lasagne.datasets import mnist, fonts, notmnist, insects, ilc\n",
      "\n",
      "import theano.tensor as T\n",
      "import theano\n",
      "import os\n",
      "import numpy as np\n",
      "import time\n",
      "import seaborn as sbn\n",
      "import copy\n",
      "\n",
      "from lasagne.layers.dense import DenseLayer\n",
      "from lasagne.layers.base import Layer\n",
      "\n",
      "from pylearn2.scripts.plot_weights import (grid_plot, \n",
      "                                           build_visualizations_by_weighted_combinations)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cross_validation\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
      "\n",
      "# init a random number generator\n",
      "rng =  MRG_RandomStreams(111222333)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_model(x_dim, num_hidden_list, y_dim, tied_weights=True):\n",
      "    \n",
      "    \"\"\"\n",
      "    This function builds a deep auto-encoder model reconstructing the input\n",
      "    and in the same time predicting the labels.\n",
      "    \n",
      "    x_dim : dimensionality of the input\n",
      "    y_dim : dimensionality of the output\n",
      "    num_hidden_list : list of nb of hidden units of each layer         \n",
      "    \"\"\"\n",
      "    l_x_in = lasagne.layers.InputLayer(\n",
      "            shape=(None, x_dim),\n",
      "        )\n",
      "    l_y_in = lasagne.layers.InputLayer(\n",
      "            shape=(None, y_dim),\n",
      "        )\n",
      "\n",
      "    \n",
      "    \n",
      "    # 1) First part of the auto-encoder : encoding part\n",
      "    \n",
      "    cur_layer = l_x_in\n",
      "    weights = []\n",
      "    for i, num_hidden in enumerate(num_hidden_list): # ?? num_hidden est \n",
      "                                            #le nbr de couches cach\u00e9es? modifi\u00e9 : num_hidden_list est une liste\n",
      "                                            # contenant le nombre d'unit\u00e9s dans \n",
      "                                            # chaque couche cach\u00e9e \n",
      "        #lasagne.init.Uniform() use by default the Glorot et al initiliazition \n",
      "        # which works well for images (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
      "        # alternatives : Normal(), Orthogonal(), Sparse(sparsity coef, standard dev of units)\n",
      "        \n",
      "        W = lasagne.init.Uniform()\n",
      "        \n",
      "        \n",
      "        # Fill the biases with zero\n",
      "        b = lasagne.init.Constant(0.)\n",
      "        \n",
      "        nonlinearity = lasagne.nonlinearities.sigmoid\n",
      "            \n",
      "        cur_layer = lasagne.layers.DenseLayer(\n",
      "            cur_layer,\n",
      "            num_units=num_hidden,\n",
      "            nonlinearity=nonlinearity,\n",
      "            W=W,\n",
      "            b=b\n",
      "        )\n",
      "        weights.append(cur_layer.W)\n",
      "    \n",
      "    l_code = cur_layer # l_code is the topest layer of the encoder\n",
      "\n",
      "    # 2) second part of the auto-encoder : the decoder\n",
      "    num_hidden_list_backward = list(reversed(num_hidden_list))[1:]\n",
      "    \n",
      "    for i, num_hidden in enumerate(num_hidden_list_backward  + [x_dim] ):\n",
      "        \n",
      "        if tied_weights == True:            \n",
      "            W = (weights[len(num_hidden_list) - i - 1].T)\n",
      "        else:\n",
      "            W = lasagne.init.Uniform()\n",
      "        \n",
      "        b = lasagne.init.Constant(0.)\n",
      "        \n",
      "        nonlinearity = lasagne.nonlinearities.sigmoid\n",
      "        \n",
      "        \n",
      "        cur_layer = DenseLayer(\n",
      "            cur_layer,\n",
      "            num_units=num_hidden,\n",
      "            nonlinearity=nonlinearity,\n",
      "            W=W,\n",
      "            b=b,\n",
      "            tied_weights=tied_weights\n",
      "        )\n",
      "    \n",
      "    \n",
      "    # l_x_hat is the reconstruction layer : ^(x)\n",
      "    l_x_hat = cur_layer\n",
      "    \n",
      "    # 3) a layer predicting the labels from the code\n",
      "    \n",
      "    \n",
      "    # l_y_hat is the  layer of label predictions\n",
      "    l_y_hat = lasagne.layers.DenseLayer( ## ?? C'est quoi denselayer?\n",
      "                                        ## fully connected? Oui dense  = fully connected\n",
      "        l_code,\n",
      "        num_units = y_dim,\n",
      "        nonlinearity=lasagne.nonlinearities.tanh\n",
      "    )\n",
      "\n",
      "    return l_x_in, l_y_in, l_code, l_x_hat, l_y_hat\n",
      "\n",
      "\n",
      "# the function that sets the loss function to minimize\n",
      "def objective(X_batch, y_batch, x_hat, y_hat, prediction_coeficient=0):\n",
      "    \"\"\"\n",
      "        X_batch : the given input\n",
      "        y_batch : the given output\n",
      "        x_hat : the reconstructed input\n",
      "        y_hat : the predicted labels\n",
      "        \n",
      "    \"\"\"\n",
      "    \n",
      "    # minimize cross-entropy (for reconstruction)\n",
      "    reconstruction_loss = -T.mean(  T.sum( (X_batch * T.log(x_hat) + (1 - X_batch) * T.log(1 - x_hat)) , axis=1 )   )\n",
      "    #reconstruction_loss = T.mean(   T.sum((X_batch - x_hat)**2, axis=1)  )\n",
      "    # minimize the prediction error\n",
      "    prediction_loss = T.mean( (y_batch - y_hat)**2  )\n",
      "    return reconstruction_loss + prediction_loss * prediction_coeficient\n",
      "\n",
      "# Zero-Masking noise : corruption_level is the ratio by which we corrupt, 0.2 means\n",
      "# 0.2 of the pixels will be randomly set to zero for each example\n",
      "def corrupted_zero_masking(rng, x, corruption_level):\n",
      "    return x * rng.binomial(size=x.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX)\n",
      "\n",
      "# Salt And pepper noise : a subset of pixels are selected randomly and transformed fairly into 1 or 0\n",
      "def corrupted_salt_and_pepper(rng, x, corruption_level):\n",
      "    selected = rng.binomial(size=x.shape, n=1, p=corruption_level, dtype=theano.config.floatX)\n",
      "    return x * (1 - selected) + selected * rng.binomial(size=x.shape, n=1, p=0.5, dtype=theano.config.floatX)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Experiment(lasagne.easy.Experiment):\n",
      "    \"\"\"\n",
      "    An experiment is an object doing all the steps we need:\n",
      "    \n",
      "      - 1) incorporating the dataset\n",
      "      - 2) setting up the hyper-parameters\n",
      "      - 3) building the model\n",
      "      - 4) run it\n",
      "      - 5) collect statistics and do some visualizations\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, x_hidden_units):\n",
      "        self.x_hidden_units = x_hidden_units\n",
      "        self.rng = rng\n",
      "        self.stats = []\n",
      "    \n",
      "    # - format the data so that labels are one-hot vectors instead of numbers\n",
      "    # - set the train, valid and test set\n",
      "    def load_data(self, datasets):\n",
      "        datasets_ = {}\n",
      "        for k, d in datasets.items():\n",
      "            d_x = d.X\n",
      "            d_y = theano.shared(lasagne.utils.floatX(lasagne.easy.to_hamming(d.y.get_value(), presence=1, absence=-1)))\n",
      "            datasets_[k] = lasagne.data.Dataset(d_x, d_y)\n",
      "        datasets = datasets_\n",
      "        \n",
      "        self.train, self.valid, self.test =  datasets[\"train\"], datasets[\"valid\"], datasets[\"test\"]\n",
      "    \n",
      "    # initialize the hyper-parameters\n",
      "    def set_hp(self, hp=None):\n",
      "        if hp is None:\n",
      "            hp = {}\n",
      "        self.learning_rate = hp.get(\"learning_rate\", 0.1)\n",
      "        self.momentum = hp.get(\"momentum\", 0.)\n",
      "# ?? what does the next to line do? - I dont get the syntax (don't understand )\n",
      "        self.input_dim = self.train.X.get_value().shape[1]\n",
      "        self.output_dim  = self.train.y.get_value().shape[1]\n",
      "        self.batch_size = hp.get(\"batch_size\", 20)\n",
      "        self.nb_batches = self.train.X.get_value().shape[0] // self.batch_size\n",
      "        self.nb_epochs = hp.get(\"nb_epochs\", 15)\n",
      "        self.corruption = hp.get(\"corruption\", 0.3)\n",
      "        self.prediction_coeficient = hp.get(\"prediction_coeficient\", 0.)\n",
      "    \n",
      "    def build_model(self):\n",
      "        \n",
      "        self.batch_index, self.X_batch, self.y_batch, self.batch_slice = easy.get_theano_batch_variables(self.batch_size, \n",
      "                                                                                                         y_softmax=False)\n",
      "        \n",
      "\n",
      "        # creates the model\n",
      "        self.l_x_in, self.l_y_in, self.l_code, self.l_x_hat, self.l_y_hat = build_model(self.input_dim, \n",
      "                                                                                        self.x_hidden_units,\n",
      "                                                                                        self.output_dim)\n",
      "        \n",
      "        # get the corrupted version of the input\n",
      "        self.corrupted_X_batch = corrupted_zero_masking(self.rng, self.X_batch, self.corruption)\n",
      "\n",
      "\n",
      "        # get the loss function\n",
      "        self.loss = objective(self.X_batch,\n",
      "                              self.y_batch,\n",
      "                              self.l_x_hat.get_output(self.corrupted_X_batch), # the reconstruction from the corrupted input,\n",
      "                              self.l_y_hat.get_output({self.l_x_in : self.X_batch, \n",
      "                                                       self.l_y_in : self.y_batch} ),# the prediction label\n",
      "                              prediction_coeficient=self.prediction_coeficient) \n",
      "        \n",
      "        # the predictions\n",
      "        pred = T.argmax(self.l_y_hat.get_output({self.l_x_in : self.X_batch, self.l_y_in : self.y_batch}),\n",
      "                        axis=1)\n",
      "        # the accuracy\n",
      "        accuracy = T.mean(T.eq(pred, T.argmax(self.y_batch, axis=1)))\n",
      "        #reconstruction_error =  T.mean( T.sum( T.abs_(self.l_x_hat.get_output(self.X_batch) - self.X_batch), axis=1 ) )\n",
      "        z = self.l_x_hat.get_output(self.X_batch)\n",
      "        x = self.X_batch\n",
      "        L = - T.sum(x * T.log(z) + (1 - x) * T.log(1 - z), axis=1)\n",
      "        reconstruction_error = T.mean(L)\n",
      "\n",
      "        # a function that computes the loss\n",
      "        self.get_loss = theano.function([self.X_batch, self.y_batch], self.loss)\n",
      "        # a function that get the reconstructions\n",
      "        self.get_reconstruction = theano.function([self.X_batch],\n",
      "                                                   self.l_x_hat.get_output(self.X_batch))\n",
      "        \n",
      "        code = T.matrix('code')\n",
      "        self.get_reconstruction_from_code = theano.function([code],\n",
      "                                                             self.l_x_hat.get_output({exp.l_code : code}) )\n",
      "        self.get_reconstruction_error = theano.function([self.X_batch],\n",
      "                                                        reconstruction_error)\n",
      "        # a function that gets the code\n",
      "        self.get_code = theano.function([self.X_batch], self.l_code.get_output(self.X_batch))\n",
      "        \n",
      "        # a function that gets the accuracy                  \n",
      "        self.get_accuracy = theano.function([self.X_batch, self.y_batch], accuracy)\n",
      "        \n",
      "\n",
      "        # get the gradient updates\n",
      "        all_params = lasagne.layers.get_all_params(self.l_x_hat)\n",
      "        \n",
      "        # uses adadelta as on optimization procedure (http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf)\n",
      "        #self.updates = lasagne.updates.adadelta(self.loss, all_params, learning_rate=self.learning_rate)\n",
      "        self.updates = lasagne.updates.nesterov_momentum(self.loss, all_params, learning_rate=self.learning_rate,\n",
      "                                                         momentum=self.momentum)\n",
      "\n",
      "        # get the iteration update\n",
      "        self.iter_update_batch = easy.get_iter_update_supervision(self.train.X, self.train.y, self.X_batch, self.y_batch,\n",
      "                                                                  self.loss, self.updates,\n",
      "                                                                  self.batch_index, self.batch_slice)\n",
      "        \n",
      "        \n",
      "    def run(self):\n",
      "        train, valid, test = self.train, self.valid, self.test\n",
      "\n",
      "        def iter_update(epoch):\n",
      "            \"\"\"\n",
      "            - this function is called in each epoch\n",
      "            - it returns (in each epoch) an update status : it is a dict containing \n",
      "              the loss, accuracy, reconstruction error of train, valid and test sets\n",
      "            \"\"\"\n",
      "    \n",
      "            # for each batch, update the parameters\n",
      "            for i in xrange(self.nb_batches):\n",
      "                self.iter_update_batch(i)\n",
      "\n",
      "            # get the loss of train, valid  and test sets\n",
      "            ds = {\"train\": train, \"valid\": valid, \"test\": test}\n",
      "            status = {}\n",
      "            for k, v in ds.items():\n",
      "                X, y = v.X.get_value(), v.y.get_value()\n",
      "                status[\"loss_\" + k] = self.get_loss(X, y)\n",
      "                status[\"accuracy_\" + k] = self.get_accuracy(X, y)\n",
      "                status[\"reconstruction_error_\" + k] = self.get_reconstruction_error(X)\n",
      "                \n",
      "            status[\"epoch\"] = epoch\n",
      "            return status\n",
      "\n",
      "        def quitter(update_status):\n",
      "            \"\"\"\n",
      "                in each epoch quitter is called to check if the program will stop before reaching the\n",
      "                given number of epochs\n",
      "            \"\"\"\n",
      "            \n",
      "            # here we don't stop\n",
      "            return False\n",
      "\n",
      "        def monitor(update_status):\n",
      "            \"\"\"\n",
      "                in each epoch, get the update_status from iter_update.\n",
      "                returns the monitor output which will be communicated to the observer.\n",
      "            \"\"\"\n",
      "            self.stats.append(update_status)\n",
      "            return update_status\n",
      "        \n",
      "        \n",
      "        def observer(monitor_output):\n",
      "            \"\"\"\n",
      "                get the status of the current epoch from the monitor and shows it\n",
      "            \"\"\"\n",
      "            if monitor_output[\"epoch\"] % 20 == 0:\n",
      "                for k, v in monitor_output.items():\n",
      "                    print(\"%s : %f\" % (k, v))\n",
      "        \n",
      "        \n",
      "        # train the model\n",
      "        lasagne.easy.main_loop(self.nb_epochs, iter_update, quitter, monitor, observer)\n",
      "    \n",
      "    # after training the model, show the reconstructions\n",
      "    def post_show_reconstructions(self, nb=30):\n",
      "        x = easy.get_2d_square_image_view(self.train.X.get_value())\n",
      "        x_rec = self.get_reconstruction(self.train.X.get_value())\n",
      "        x_rec = easy.get_2d_square_image_view(x_rec)\n",
      "        \n",
      "        indices = np.random.randint(0, x.shape[0] - 1, size=(nb,))\n",
      "\n",
      "        k = 1\n",
      "        for i in xrange(nb):\n",
      "            plt.subplot(nb, 2, k)\n",
      "            plt.axis('off')\n",
      "            plt.imshow(x[indices[i]], cmap='gray')\n",
      "            k += 1\n",
      "            plt.subplot(nb, 2, k)\n",
      "            plt.axis('off')\n",
      "            plt.imshow(x_rec[indices[i]], cmap='gray')\n",
      "            k += 1\n",
      "        plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layerwise_autoencoders_hidden = [1000, 1000, 500, 500, 300, 128, 32]\n",
      "\n",
      "# Load  dataset\n",
      "dataset = lasagne.datasets.fonts.Fonts (kind='all_32', labels_kind='letters')  # change by lasagne.datasets.datasets.mnist.MNIST\n",
      "                                         # or by lasagne.datasets.fonts.Fonts with (kind='all_32', labels_kind='letters')\n",
      "                                         # or by lasagne.datasets.notmnist.NotMNIST\n",
      "                                         # or by lasagne.datasets.insects.Insects \n",
      "                                         # or by lasagne.datasets.ilc.ILC\n",
      "dataset.load()\n",
      "mean = dataset.X.mean(axis=0)[np.newaxis, :]\n",
      "std = dataset.X.std(axis=0)[np.newaxis, :]\n",
      "\n",
      "# standardize\n",
      "#dataset.X = (dataset.X - mean) / std\n",
      "\n",
      "# Split\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset.X, dataset.y, test_size=0.25)\n",
      "X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(X_train, y_train, test_size=0.25)\n",
      "\n",
      "X_train = theano.shared(X_train)\n",
      "X_valid = theano.shared(X_valid)\n",
      "X_test = theano.shared(X_test)\n",
      "y_train = theano.shared(y_train)\n",
      "y_valid = theano.shared(y_valid)\n",
      "y_test = theano.shared(y_test)\n",
      "\n",
      "\n",
      "datasets = {\"train\" : lasagne.data.Dataset(X_train, y_train), \n",
      "            \"valid\": lasagne.data.Dataset(X_valid, y_valid),\n",
      "            \"test\": lasagne.data.Dataset(X_test, y_test)}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualize the dataset\n",
      "grid_plot(lasagne.easy.get_2d_square_image_view(datasets['train'].X.get_value()), nbrows=10, nbcols=10, random=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDDVBEITOIUNNEAShc8hQEwRB6Jz/DzsBj2cHzc5NAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f18658dcf10>"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# un experiment c'est un entrainment d'un auto-encoder.\n",
      "# dans chaque experiment on entraine un auto-encoder pour reconstruire la couche precedente (greedy layer-wise auto-encoders)\n",
      "# apres chaque expriment on transforme le dataset en utilisant l'auto-encoder qu'on a entrain\u00e9\n",
      "# On garde les data g\u00e9n\u00e9r\u00e9s pour chaque couche\n",
      "\n",
      "# ?? que veut dire: reconstruction_error_wrong_pixels_per_example_train, rennom\u00e9e en reconstruction_error\n",
      "# je voulais juste dire que c'est le nombre de pixels erron\u00e9s en moyenne pour chaque exemple\n",
      "\n",
      "# initialize the experiments\n",
      "reconstruct_previous_layer_experiments = []\n",
      "for nb_hidden in layerwise_autoencoders_hidden:\n",
      "    exp = Experiment([nb_hidden])\n",
      "    reconstruct_previous_layer_experiments.append(exp)\n",
      "\n",
      "cur_datasets = datasets\n",
      "for i, exp in enumerate(reconstruct_previous_layer_experiments):\n",
      "    print \"Training the layer %d\" % (i,)\n",
      "    # Reconstruct the previous layer (for the first experiment, reconstruct the input)\n",
      "    exp.load_data(cur_datasets)\n",
      "    hp = {\n",
      "        \"nb_epochs\": 30,\n",
      "        \"corruption\": 0.0,\n",
      "        \"batch_size\": 20,\n",
      "        \"learning_rate\": 0.1,\n",
      "        \"momentum\": 0.\n",
      "    }\n",
      "    exp.set_hp(hp)\n",
      "    exp.build_model()\n",
      "    exp.run()\n",
      "    # transform the datasets splits using the new learned auto-encoder\n",
      "    new_datasets = {}\n",
      "    for name, dataset in cur_datasets.items():\n",
      "        X, y = dataset.X, dataset.y\n",
      "        X = theano.shared(exp.get_code(X.get_value()))\n",
      "        new_datasets[name] = lasagne.data.Dataset(X, y)\n",
      "    cur_datasets = new_datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the layer 0\n",
        "accuracy_train : 0.058630"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loss_test : 38.380722\n",
        "reconstruction_error_test : 38.380798\n",
        "loss_valid : 37.596554\n",
        "loss_train : 35.477779\n",
        "epoch : 0.000000\n",
        "accuracy_valid : 0.055865\n",
        "accuracy_test : 0.060523\n",
        "reconstruction_error_train : 35.478199\n",
        "reconstruction_error_valid : 37.596596\n",
        "accuracy_train : 0.056879"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loss_test : 6.474827\n",
        "reconstruction_error_test : 6.475147\n",
        "loss_valid : 6.191061\n",
        "loss_train : 1.867059\n",
        "epoch : 20.000000\n",
        "accuracy_valid : 0.063505\n",
        "accuracy_test : 0.065775\n",
        "reconstruction_error_train : 1.867237\n",
        "reconstruction_error_valid : 6.191270\n",
        "Training the layer 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy_train : 0.041121"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loss_test : 381.968414\n",
        "reconstruction_error_test : 381.969025\n",
        "loss_valid : 382.882538\n",
        "loss_train : 377.685242\n",
        "epoch : 0.000000\n",
        "accuracy_valid : 0.044724\n",
        "accuracy_test : 0.043572\n",
        "reconstruction_error_train : 377.687164\n",
        "reconstruction_error_valid : 382.882965\n",
        "accuracy_train : 0.034329"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loss_test : 325.453369\n",
        "reconstruction_error_test : 325.454407\n",
        "loss_valid : 326.523224\n",
        "loss_train : 316.606567\n",
        "epoch : 20.000000\n",
        "accuracy_valid : 0.034219\n",
        "accuracy_test : 0.035096\n",
        "reconstruction_error_train : 316.609589\n",
        "reconstruction_error_valid : 326.523804\n",
        "Training the layer 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "MemoryError",
       "evalue": "Error allocating 75388000 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuElemwise{Composite{[scalar_sigmoid(add(i0, i1))]}}[(0, 0)].0, GpuDimShuffle{1,0}.0)\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(18847, 500), (500, 1000)]\nInputs strides: [(500, 1), (1, 500)]\nInputs values: ['not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-41-e763225b00d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_hp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;31m# transform the datasets splits using the new learned auto-encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnew_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-37-1062dc563824>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0measy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# after training the model, show the reconstructions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/gridcl/mehdicherti/work/code/Lasagne/lasagne/easy.pyc\u001b[0m in \u001b[0;36mmain_loop\u001b[1;34m(max_nb_epochs, iter_update, quitter, monitor, observer)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_nb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_nb_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mupdate_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0mmonitor_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mobserver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-37-1062dc563824>\u001b[0m in \u001b[0;36miter_update\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reconstruction_error_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epoch\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/gridcl/mehdicherti/anaconda/lib/python2.7/site-packages/Theano-0.6.0-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/gridcl/mehdicherti/anaconda/lib/python2.7/site-packages/Theano-0.6.0-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mMemoryError\u001b[0m: Error allocating 75388000 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuElemwise{Composite{[scalar_sigmoid(add(i0, i1))]}}[(0, 0)].0, GpuDimShuffle{1,0}.0)\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(18847, 500), (500, 1000)]\nInputs strides: [(500, 1), (1, 500)]\nInputs values: ['not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build the stats for plots\n",
      "\n",
      "splits = (\"train\", \"valid\", \"test\")\n",
      "\n",
      "def get_stats(experiment, stat_names):\n",
      "    stats = {}\n",
      "    for spl in splits:\n",
      "        stats[spl] = {}\n",
      "        for v in stat_names:\n",
      "            stats[spl][v] = [stat[v + \"_\" + spl] for stat in experiment.stats]\n",
      "    return stats\n",
      "\n",
      "def plot_stat(stats, stat_name):\n",
      "    for spl in splits:\n",
      "        plt.plot(stats[spl][stat_name], label=\"%s %s\" % (spl, stat_name))\n",
      "    plt.legend(bbox_to_anchor=(0.5, 1.05))\n",
      "    plt.xlabel(\"epoch\")\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "stat_names = [\"accuracy\", \"loss\", \"reconstruction_error\"]\n",
      "stats = get_stats(reconstruct_previous_layer_experiments[0], stat_names)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"accuracy\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"reconstruction_error\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"loss\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the weights of each layer\n",
      "weights = []\n",
      "for exp in reconstruct_previous_layer_experiments:\n",
      "    all_params = lasagne.layers.get_all_params(exp.l_x_hat)\n",
      "    W = all_params[1].get_value().T # get te weight matrix\n",
      "    weights.append(W)\n",
      "\n",
      "#build the visualizations of each layer\n",
      "vis = build_visualizations_by_weighted_combinations(weights, until_layer=len(reconstruct_previous_layer_experiments)-1, top=1.)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# une qui genere le visualisations recursivement : le visualisation d'une unit\u00e9 A du layer N\n",
      "# est la moyenne des visualizations de X unit\u00e9s du layer N - 1 avec lesquelles\n",
      "#  l'unit\u00e9 A a la plus grosse connectivit\u00e9 (en termes de poids reliant A aux couches du layer precedent)\n",
      "# La deuxieme m\u00e9thode prend la moyenne des top X examples qui maximisent l'activation de l'unit\u00e9\n",
      "\n",
      "# ?? Peut-on \u00e9chantillonner et afficher les \u00e9chantillons? Faut \u00e9ventuellement les faire.\n",
      "\n",
      "# Visualize the first layer\n",
      "grid_plot(vis[0], imshow_options={\"cmap\": \"gray\"}, nbrows=20, nbcols=20, random=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show reconstructions\n",
      "reconstruct_previous_layer_experiments[0].post_show_reconstructions(nb=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the second layer (if available)\n",
      "grid_plot(vis[1], imshow_options={\"cmap\": \"gray\"}, nbrows=20, nbcols=20, random=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the third layer (if available)\n",
      "grid_plot(vis[2], imshow_options={\"cmap\": \"gray\"}, nbrows=20, nbcols=20, random=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute the representations of each layer\n",
      "dataset = datasets['test']\n",
      "X = dataset.X.get_value()\n",
      "representations = [X]\n",
      "for exp in reconstruct_previous_layer_experiments:\n",
      "    X = exp.get_code(X)\n",
      "    representations.append(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Plot the distributions of the representation of a given layer\n",
      "# Randomly selection of a set of features in the representation and\n",
      "# plot in a grid the following:\n",
      "# - For non-diagonal locations, plot relationship between two features\n",
      "# - For diagonal ones, plot the histogram of a feature\n",
      "# The id of the selected features is in \"selected_features\"\n",
      "\n",
      "# If we want \"easy\" sampling with auto-encoders, we need the last layer to be uniform...\n",
      "\n",
      "layer = 5\n",
      "selected_features = np.random.randint(0, reprenstation.shape[1] - 1, size=5)\n",
      "\n",
      "representation = representations[layer]\n",
      "reprenstation = pd.DataFrame(reprenstation)\n",
      "g = sbn.PairGrid(reprenstation, vars=selected_features)\n",
      "g.map_diag(plt.hist)\n",
      "g.map_offdiag(plt.scatter)\n",
      "g.add_legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or we can model the last layer representation using an RBM (a simple one if possible)...\n",
      "\n",
      "from sklearn.neural_network.rbm import BernoulliRBM\n",
      "representation = representations[-1]\n",
      "print reprenstation.shape\n",
      "model = BernoulliRBM(n_components=80, n_iter=100)\n",
      "model.fit(representation)\n",
      "\n",
      "def sample_from_rbm(rbm, nb_examples, nb_iterations=10):\n",
      "    nb_features = rbm.components_.shape[1]\n",
      "    v = np.random.uniform(size=(nb_examples, nb_features))\n",
      "    for i in xrange(nb_iterations):\n",
      "        v = rbm.gibbs(v)\n",
      "    return v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The sampling  part\n",
      "\n",
      "\n",
      "nb_examples = 200 # nb of examples to sample\n",
      "nb_iterations = 300 # nb of gibbs iterations\n",
      "\n",
      "# sample from RBM\n",
      "X = sample_from_rbm(model, nb_examples, nb_iterations)\n",
      "\n",
      "# go back to the input layer\n",
      "for exp in reversed(reconstruct_previous_layer_experiments):\n",
      "    \n",
      "    code = T.matrix('code')\n",
      "    get_reconstruction_from_code = theano.function([code],\n",
      "                                                    exp.l_x_hat.get_output({exp.l_code : code}) )\n",
      "    X  = get_reconstruction_from_code(X)\n",
      "    X = exp.get_reconstruction(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Show the samples\n",
      "grid_plot(lasagne.easy.get_2d_square_image_view(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}