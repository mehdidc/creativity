{
 "metadata": {
  "name": "",
  "signature": "sha256:10696fb47a168d2e6ce437d47030c6997d30bcb977a35568fc2a4c9cc51d0156"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#%load_ext autoreload\n",
      "\n",
      "#%autoreload 2\n",
      "\n",
      "from lasagne import easy\n",
      "import lasagne\n",
      "import lasagne.data\n",
      "\n",
      "# currently available datasets:\n",
      "from lasagne.datasets import mnist, fonts, notmnist, insects, ilc\n",
      "\n",
      "import theano.tensor as T\n",
      "import theano\n",
      "import os\n",
      "import numpy as np\n",
      "import time\n",
      "import seaborn as sbn\n",
      "import copy\n",
      "\n",
      "from lasagne.layers.dense import DenseLayer\n",
      "from lasagne.layers.base import Layer\n",
      "\n",
      "from pylearn2.scripts.plot_weights import (grid_plot, \n",
      "                                           build_visualizations_by_weighted_combinations)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cross_validation\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
      "\n",
      "# init a random number generator\n",
      "rng =  MRG_RandomStreams(111222333)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 1: Tesla K20m\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_model(x_dim, num_hidden_list, y_dim, tied_weights=True):\n",
      "    \n",
      "    \"\"\"\n",
      "    This function builds a deep auto-encoder model reconstructing the input\n",
      "    and in the same time predicting the labels.\n",
      "    \n",
      "    x_dim : dimensionality of the input\n",
      "    y_dim : dimensionality of the output\n",
      "    num_hidden_list : list of nb of hidden units of each layer         \n",
      "    \"\"\"\n",
      "    l_x_in = lasagne.layers.InputLayer(\n",
      "            shape=(None, x_dim),\n",
      "        )\n",
      "    l_y_in = lasagne.layers.InputLayer(\n",
      "            shape=(None, y_dim),\n",
      "        )\n",
      "\n",
      "    \n",
      "    \n",
      "    # 1) First part of the auto-encoder : encoding part\n",
      "    \n",
      "    cur_layer = l_x_in\n",
      "    weights = []\n",
      "    for i, num_hidden in enumerate(num_hidden_list): # ?? num_hidden est \n",
      "                                            #le nbr de couches cach\u00e9es? modifi\u00e9 : num_hidden_list est une liste\n",
      "                                            # contenant le nombre d'unit\u00e9s dans \n",
      "                                            # chaque couche cach\u00e9e \n",
      "        #lasagne.init.Uniform() use by default the Glorot et al initiliazition \n",
      "        # which works well for images (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
      "        # alternatives : Normal(), Orthogonal(), Sparse(sparsity coef, standard dev of units)\n",
      "        \n",
      "        W = lasagne.init.Uniform()\n",
      "        \n",
      "        \n",
      "        # Fill the biases with zero\n",
      "        b = lasagne.init.Constant(0.)\n",
      "        \n",
      "        nonlinearity = lasagne.nonlinearities.sigmoid\n",
      "            \n",
      "        cur_layer = lasagne.layers.DenseLayer(\n",
      "            cur_layer,\n",
      "            num_units=num_hidden,\n",
      "            nonlinearity=nonlinearity,\n",
      "            W=W,\n",
      "            b=b\n",
      "        )\n",
      "        weights.append(cur_layer.W)\n",
      "    \n",
      "    l_code = cur_layer # l_code is the topest layer of the encoder\n",
      "\n",
      "    # 2) second part of the auto-encoder : the decoder\n",
      "    num_hidden_list_backward = list(reversed(num_hidden_list))[1:]\n",
      "    \n",
      "    for i, num_hidden in enumerate(num_hidden_list_backward  + [x_dim] ):\n",
      "        \n",
      "        if tied_weights == True:            \n",
      "            W = (weights[len(num_hidden_list) - i - 1].T)\n",
      "        else:\n",
      "            W = lasagne.init.Uniform()\n",
      "        \n",
      "        b = lasagne.init.Constant(0.)\n",
      "        \n",
      "        nonlinearity = lasagne.nonlinearities.sigmoid\n",
      "        \n",
      "        \n",
      "        cur_layer = DenseLayer(\n",
      "            cur_layer,\n",
      "            num_units=num_hidden,\n",
      "            nonlinearity=nonlinearity,\n",
      "            W=W,\n",
      "            b=b,\n",
      "            tied_weights=tied_weights\n",
      "        )\n",
      "    \n",
      "    \n",
      "    # l_x_hat is the reconstruction layer : ^(x)\n",
      "    l_x_hat = cur_layer\n",
      "    \n",
      "    # 3) a layer predicting the labels from the code\n",
      "    \n",
      "    \n",
      "    # l_y_hat is the  layer of label predictions\n",
      "    l_y_hat = lasagne.layers.DenseLayer( ## ?? C'est quoi denselayer?\n",
      "                                        ## fully connected? Oui dense  = fully connected\n",
      "        l_code,\n",
      "        num_units = y_dim,\n",
      "        nonlinearity=lasagne.nonlinearities.tanh\n",
      "    )\n",
      "\n",
      "    return l_x_in, l_y_in, l_code, l_x_hat, l_y_hat\n",
      "\n",
      "\n",
      "# the function that sets the loss function to minimize\n",
      "def objective(X_batch, y_batch, x_hat, y_hat, prediction_coeficient=0):\n",
      "    \"\"\"\n",
      "        X_batch : the given input\n",
      "        y_batch : the given output\n",
      "        x_hat : the reconstructed input\n",
      "        y_hat : the predicted labels\n",
      "        \n",
      "    \"\"\"\n",
      "    \n",
      "    # minimize cross-entropy (for reconstruction)\n",
      "    reconstruction_loss = -T.mean(  T.sum( (X_batch * T.log(x_hat) + (1 - X_batch) * T.log(1 - x_hat)) , axis=1 )   )\n",
      "    #reconstruction_loss = T.mean(   T.sum((X_batch - x_hat)**2, axis=1)  )\n",
      "    # minimize the prediction error\n",
      "    prediction_loss = T.mean( (y_batch - y_hat)**2  )\n",
      "    return reconstruction_loss + prediction_loss * prediction_coeficient\n",
      "\n",
      "# Zero-Masking noise : corruption_level is the ratio by which we corrupt, 0.2 means\n",
      "# 0.2 of the pixels will be randomly set to zero for each example\n",
      "def corrupted_zero_masking(rng, x, corruption_level):\n",
      "    return x * rng.binomial(size=x.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX)\n",
      "\n",
      "# Salt And pepper noise : a subset of pixels are selected randomly and transformed fairly into 1 or 0\n",
      "def corrupted_salt_and_pepper(rng, x, corruption_level):\n",
      "    selected = rng.binomial(size=x.shape, n=1, p=corruption_level, dtype=theano.config.floatX)\n",
      "    return x * (1 - selected) + selected * rng.binomial(size=x.shape, n=1, p=0.5, dtype=theano.config.floatX)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Experiment(lasagne.easy.Experiment):\n",
      "    \"\"\"\n",
      "    An experiment is an object doing all the steps we need:\n",
      "    \n",
      "      - 1) incorporating the dataset\n",
      "      - 2) setting up the hyper-parameters\n",
      "      - 3) building the model\n",
      "      - 4) run it\n",
      "      - 5) collect statistics and do some visualizations\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, x_hidden_units):\n",
      "        self.x_hidden_units = x_hidden_units\n",
      "        self.rng = rng\n",
      "        self.stats = []\n",
      "    \n",
      "    # - format the data so that labels are one-hot vectors instead of numbers\n",
      "    # - set the train, valid and test set\n",
      "    def load_data(self, datasets):\n",
      "        datasets_ = {}\n",
      "        for k, d in datasets.items():\n",
      "            d_x = d.X\n",
      "            d_y = theano.shared(lasagne.utils.floatX(lasagne.easy.to_hamming(d.y.get_value(), presence=1, absence=-1)))\n",
      "            datasets_[k] = lasagne.data.Dataset(d_x, d_y)\n",
      "        datasets = datasets_\n",
      "        \n",
      "        self.train, self.valid, self.test =  datasets[\"train\"], datasets[\"valid\"], datasets[\"test\"]\n",
      "    \n",
      "    # initialize the hyper-parameters\n",
      "    def set_hp(self, hp=None):\n",
      "        if hp is None:\n",
      "            hp = {}\n",
      "        self.learning_rate = hp.get(\"learning_rate\", 0.1)\n",
      "        self.momentum = hp.get(\"momentum\", 0.)\n",
      "# ?? what does the next to line do? - I dont get the syntax (don't understand )\n",
      "        self.input_dim = self.train.X.get_value().shape[1]\n",
      "        self.output_dim  = self.train.y.get_value().shape[1]\n",
      "        self.batch_size = hp.get(\"batch_size\", 20)\n",
      "        self.nb_batches = self.train.X.get_value().shape[0] // self.batch_size\n",
      "        self.nb_epochs = hp.get(\"nb_epochs\", 15)\n",
      "        self.corruption = hp.get(\"corruption\", 0.3)\n",
      "        self.prediction_coeficient = hp.get(\"prediction_coeficient\", 0.)\n",
      "    \n",
      "    def build_model(self):\n",
      "        \n",
      "        self.batch_index, self.X_batch, self.y_batch, self.batch_slice = easy.get_theano_batch_variables(self.batch_size, \n",
      "                                                                                                         y_softmax=False)\n",
      "        \n",
      "\n",
      "        # creates the model\n",
      "        self.l_x_in, self.l_y_in, self.l_code, self.l_x_hat, self.l_y_hat = build_model(self.input_dim, \n",
      "                                                                                        self.x_hidden_units,\n",
      "                                                                                        self.output_dim)\n",
      "        \n",
      "        # get the corrupted version of the input\n",
      "        self.corrupted_X_batch = corrupted_zero_masking(self.rng, self.X_batch, self.corruption)\n",
      "\n",
      "\n",
      "        # get the loss function\n",
      "        self.loss = objective(self.X_batch,\n",
      "                              self.y_batch,\n",
      "                              self.l_x_hat.get_output(self.corrupted_X_batch), # the reconstruction from the corrupted input,\n",
      "                              self.l_y_hat.get_output({self.l_x_in : self.X_batch, \n",
      "                                                       self.l_y_in : self.y_batch} ),# the prediction label\n",
      "                              prediction_coeficient=self.prediction_coeficient) \n",
      "        \n",
      "        # the predictions\n",
      "        pred = T.argmax(self.l_y_hat.get_output({self.l_x_in : self.X_batch, self.l_y_in : self.y_batch}),\n",
      "                        axis=1)\n",
      "        # the accuracy\n",
      "        accuracy = T.mean(T.eq(pred, T.argmax(self.y_batch, axis=1)))\n",
      "        #reconstruction_error =  T.mean( T.sum( T.abs_(self.l_x_hat.get_output(self.X_batch) - self.X_batch), axis=1 ) )\n",
      "        z = self.l_x_hat.get_output(self.X_batch)\n",
      "        x = self.X_batch\n",
      "        L = - T.sum(x * T.log(z) + (1 - x) * T.log(1 - z), axis=1)\n",
      "        reconstruction_error = T.mean(L)\n",
      "\n",
      "        # a function that computes the loss\n",
      "        self.get_loss = theano.function([self.X_batch, self.y_batch], self.loss)\n",
      "        # a function that get the reconstructions\n",
      "        self.get_reconstruction = theano.function([self.X_batch],\n",
      "                                                   self.l_x_hat.get_output(self.X_batch))\n",
      "        self.get_reconstruction_error = theano.function([self.X_batch],\n",
      "                                                        reconstruction_error)\n",
      "        # a function that gets the code\n",
      "        self.get_code = theano.function([self.X_batch], self.l_code.get_output(self.X_batch))\n",
      "        \n",
      "        # a function that gets the accuracy                  \n",
      "        self.get_accuracy = theano.function([self.X_batch, self.y_batch], accuracy)\n",
      "        \n",
      "\n",
      "        # get the gradient updates\n",
      "        all_params = lasagne.layers.get_all_params(self.l_x_hat)\n",
      "        \n",
      "        # uses adadelta as on optimization procedure (http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf)\n",
      "        #self.updates = lasagne.updates.adadelta(self.loss, all_params, learning_rate=self.learning_rate)\n",
      "        self.updates = lasagne.updates.nesterov_momentum(self.loss, all_params, learning_rate=self.learning_rate,\n",
      "                                                         momentum=self.momentum)\n",
      "\n",
      "        # get the iteration update\n",
      "        self.iter_update_batch = easy.get_iter_update_supervision(self.train.X, self.train.y, self.X_batch, self.y_batch,\n",
      "                                                                  self.loss, self.updates,\n",
      "                                                                  self.batch_index, self.batch_slice)\n",
      "        \n",
      "        \n",
      "    def run(self):\n",
      "        train, valid, test = self.train, self.valid, self.test\n",
      "\n",
      "        def iter_update(epoch):\n",
      "            \"\"\"\n",
      "            - this function is called in each epoch\n",
      "            - it returns (in each epoch) an update status : it is a dict containing \n",
      "              the loss, accuracy, reconstruction error of train, valid and test sets\n",
      "            \"\"\"\n",
      "    \n",
      "            # for each batch, update the parameters\n",
      "            for i in xrange(self.nb_batches):\n",
      "                self.iter_update_batch(i)\n",
      "\n",
      "            # get the loss of train, valid  and test sets\n",
      "            ds = {\"train\": train, \"valid\": valid, \"test\": test}\n",
      "            status = {}\n",
      "            for k, v in ds.items():\n",
      "                X, y = v.X.get_value(), v.y.get_value()\n",
      "                status[\"loss_\" + k] = self.get_loss(X, y)\n",
      "                status[\"accuracy_\" + k] = self.get_accuracy(X, y)\n",
      "                status[\"reconstruction_error_\" + k] = self.get_reconstruction_error(X)\n",
      "                \n",
      "            status[\"epoch\"] = epoch\n",
      "            return status\n",
      "\n",
      "        def quitter(update_status):\n",
      "            \"\"\"\n",
      "                in each epoch quitter is called to check if the program will stop before reaching the\n",
      "                given number of epochs\n",
      "            \"\"\"\n",
      "            \n",
      "            # here we don't stop\n",
      "            return False\n",
      "\n",
      "        def monitor(update_status):\n",
      "            \"\"\"\n",
      "                in each epoch, get the update_status from iter_update.\n",
      "                returns the monitor output which will be communicated to the observer.\n",
      "            \"\"\"\n",
      "            self.stats.append(update_status)\n",
      "            return update_status\n",
      "        \n",
      "        \n",
      "        def observer(monitor_output):\n",
      "            \"\"\"\n",
      "                get the status of the current epoch from the monitor and shows it\n",
      "            \"\"\"\n",
      "            if monitor_output[\"epoch\"] % 20 == 0:\n",
      "                for k, v in monitor_output.items():\n",
      "                    print(\"%s : %f\" % (k, v))\n",
      "        \n",
      "        \n",
      "        # train the model\n",
      "        lasagne.easy.main_loop(self.nb_epochs, iter_update, quitter, monitor, observer)\n",
      "    \n",
      "    # after training the model, show the reconstructions\n",
      "    def post_show_reconstructions(self, nb=30):\n",
      "        x = easy.get_2d_square_image_view(self.train.X.get_value())\n",
      "        x_rec = self.get_reconstruction(self.train.X.get_value())\n",
      "        x_rec = easy.get_2d_square_image_view(x_rec)\n",
      "        \n",
      "        indices = np.random.randint(0, x.shape[0] - 1, size=(nb,))\n",
      "\n",
      "        k = 1\n",
      "        for i in xrange(nb):\n",
      "            plt.subplot(nb, 2, k)\n",
      "            plt.axis('off')\n",
      "            plt.imshow(x[indices[i]], cmap='gray')\n",
      "            k += 1\n",
      "            plt.subplot(nb, 2, k)\n",
      "            plt.axis('off')\n",
      "            plt.imshow(x_rec[indices[i]], cmap='gray')\n",
      "            k += 1\n",
      "        plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layerwise_autoencoders_hidden = [1000, 1000, 500]\n",
      "\n",
      "# Load  dataset\n",
      "dataset = lasagne.datasets.notmnist.NotMNIST  # change by lasagne.datasets.datasets.mnist.MNIST\n",
      "                                         # or by lasagne.datasets.fonts.Fonts with (kind='all_32', labels_kind='letters')\n",
      "                                         # or by lasagne.datasets.notmnist.NotMNIST\n",
      "                                         # or by lasagne.datasets.insects.Insects \n",
      "                                         # or by lasagne.datasets.ilc.ILC\n",
      "dataset.load()\n",
      "mean = dataset.X.mean(axis=0)[np.newaxis, :]\n",
      "std = dataset.X.std(axis=0)[np.newaxis, :]\n",
      "\n",
      "# standardize\n",
      "#dataset.X = (dataset.X - mean) / std\n",
      "\n",
      "# Split\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset.X, dataset.y, test_size=0.25)\n",
      "X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(X_train, y_train, test_size=0.25)\n",
      "\n",
      "X_train = theano.shared(X_train)\n",
      "X_valid = theano.shared(X_valid)\n",
      "X_test = theano.shared(X_test)\n",
      "y_train = theano.shared(y_train)\n",
      "y_valid = theano.shared(y_valid)\n",
      "y_test = theano.shared(y_test)\n",
      "\n",
      "\n",
      "datasets = {\"train\" : lasagne.data.Dataset(X_train, y_train), \n",
      "            \"valid\": lasagne.data.Dataset(X_valid, y_valid),\n",
      "            \"test\": lasagne.data.Dataset(X_test, y_test)}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unbound method load() must be called with NotMNIST instance as first argument (got nothing instead)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-90f6d7957dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                                          \u001b[1;31m# or by lasagne.datasets.insects.Insects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                          \u001b[1;31m# or by lasagne.datasets.ilc.ILC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: unbound method load() must be called with NotMNIST instance as first argument (got nothing instead)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualize the dataset\n",
      "grid_plot(lasagne.easy.get_2d_square_image_view(datasets['train'].X.get_value()), nbrows=10, nbcols=10, random=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# un experiment c'est un entrainment d'un auto-encoder.\n",
      "# dans chaque experiment on entraine un auto-encoder pour reconstruire la couche precedente (greedy layer-wise auto-encoders)\n",
      "# apres chaque expriment on transforme le dataset en utilisant l'auto-encoder qu'on a entrain\u00e9\n",
      "# On garde les data g\u00e9n\u00e9r\u00e9s pour chaque couche\n",
      "\n",
      "# ?? que veut dire: reconstruction_error_wrong_pixels_per_example_train, rennom\u00e9e en reconstruction_error\n",
      "# je voulais juste dire que c'est le nombre de pixels erron\u00e9s en moyenne pour chaque exemple\n",
      "\n",
      "# initialize the experiments\n",
      "reconstruct_previous_layer_experiments = []\n",
      "for nb_hidden in layerwise_autoencoders_hidden:\n",
      "    exp = Experiment([nb_hidden])\n",
      "    reconstruct_previous_layer_experiments.append(exp)\n",
      "\n",
      "cur_datasets = datasets\n",
      "for i, exp in enumerate(reconstruct_previous_layer_experiments):\n",
      "    print \"Training the layer %d\" % (i,)\n",
      "    # Reconstruct the previous layer (for the first experiment, reconstruct the input)\n",
      "    exp.load_data(cur_datasets)\n",
      "    hp = {\n",
      "        \"nb_epochs\": 30,\n",
      "        \"corruption\": 0.3,\n",
      "        \"batch_size\": 20,\n",
      "        \"learning_rate\": 0.1,\n",
      "        \"momentum\": 0.\n",
      "    }\n",
      "    exp.set_hp(hp)\n",
      "    exp.build_model()\n",
      "    exp.run()\n",
      "    # transform the datasets splits using the new learned auto-encoder\n",
      "    new_datasets = {}\n",
      "    for name, dataset in cur_datasets.items():\n",
      "        X, y = dataset.X, dataset.y\n",
      "        X = theano.shared(exp.get_code(X.get_value()))\n",
      "        new_datasets[name] = lasagne.data.Dataset(X, y)\n",
      "    cur_datasets = new_datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build the stats for plots\n",
      "\n",
      "splits = (\"train\", \"valid\", \"test\")\n",
      "\n",
      "def get_stats(experiment, stat_names):\n",
      "    stats = {}\n",
      "    for spl in splits:\n",
      "        stats[spl] = {}\n",
      "        for v in stat_names:\n",
      "            stats[spl][v] = [stat[v + \"_\" + spl] for stat in experiment.stats]\n",
      "    return stats\n",
      "\n",
      "def plot_stat(stats, stat_name):\n",
      "    for spl in splits:\n",
      "        plt.plot(stats[spl][stat_name], label=\"%s %s\" % (spl, stat_name))\n",
      "    plt.legend(bbox_to_anchor=(0.5, 1.05))\n",
      "    plt.xlabel(\"epoch\")\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "stat_names = [\"accuracy\", \"loss\", \"reconstruction_error\"]\n",
      "stats = get_stats(reconstruct_previous_layer_experiments[0], stat_names)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"accuracy\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"reconstruction_error\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_stat(stats, \"loss\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the weights of each layer\n",
      "weights = []\n",
      "for exp in reconstruct_previous_layer_experiments:\n",
      "    all_params = lasagne.layers.get_all_params(exp.l_x_hat)\n",
      "    W = all_params[1].get_value().T # get te weight matrix\n",
      "    weights.append(W)\n",
      "\n",
      "#build the visualizations of each layer\n",
      "vis = build_visualizations_by_weighted_combinations(weights, until_layer=len(reconstruct_previous_layer_experiments)-1, top=0.01)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# une qui genere le visualisations recursivement : le visualisation d'une unit\u00e9 A du layer N\n",
      "# est la moyenne des visualizations de X unit\u00e9s du layer N - 1 avec lesquelles\n",
      "#  l'unit\u00e9 A a la plus grosse connectivit\u00e9 (en termes de poids reliant A aux couches du layer precedent)\n",
      "# La deuxieme m\u00e9thode prend la moyenne des top X examples qui maximisent l'activation de l'unit\u00e9\n",
      "\n",
      "# ?? Peut-on \u00e9chantillonner et afficher les \u00e9chantillons? Faut \u00e9ventuellement les faire.\n",
      "\n",
      "# Visualize the first layer\n",
      "grid_plot(vis[0], imshow_options={\"cmap\": \"gray\"}, nbrows=10, nbcols=10, random=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show reconstructions\n",
      "reconstruct_previous_layer_experiments[0].post_show_reconstructions(nb=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the second layer (if available)\n",
      "grid_plot(vis[1], imshow_options={\"cmap\": \"gray\"}, nbrows=10, nbcols=10, random=True)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the third layer (if available)\n",
      "grid_plot(vis[2], imshow_options={\"cmap\": \"gray\"}, nbrows=10, nbcols=10, random=True)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}